{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (4.3.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim) (1.22.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn) (1.22.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from joblib import dump\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим все необходимые данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv(\"movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 212973 entries, 0 to 212972\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   Sentence  212344 non-null  object\n",
      " 1   Movie     212973 non-null  object\n",
      " 2   Level     212973 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 4.9+ MB\n"
     ]
    }
   ],
   "source": [
    "movies_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Sentence  \\\n",
      "0                                        little girl   \n",
      "1                 don't be afraid, okay? little girl   \n",
      "2                         panting growling oh my god   \n",
      "3  theme music playing police radio chatter man w...   \n",
      "4  man i never met a woman who knew how to turn o...   \n",
      "\n",
      "                                           Movie Level  \n",
      "0  The Walking Dead-S01E01-Days Gone Bye.English    A2  \n",
      "1  The Walking Dead-S01E01-Days Gone Bye.English    A2  \n",
      "2  The Walking Dead-S01E01-Days Gone Bye.English    A2  \n",
      "3  The Walking Dead-S01E01-Days Gone Bye.English    A2  \n",
      "4  The Walking Dead-S01E01-Days Gone Bye.English    A2  \n"
     ]
    }
   ],
   "source": [
    "print(movies_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные загружены. У нас есть столбцы с текстами из фильмов, название и сам уровень. Можно приступать к предобработке."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(197772, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# удалим дубликаты \n",
    "movies_df = movies_df.drop_duplicates()\n",
    "movies_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence    107\n",
       "Movie         0\n",
       "Level         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление пропущенных значений\n",
    "movies_df = movies_df.dropna(subset=[\"Sentence\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для более-менее сбалансированной выборки возьмём определенное количество текстов каждого уровня."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание новой таблицы с заданным количеством строк для каждого уровня\n",
    "num_rows_per_level = 37000\n",
    "levels = ['A2', 'B1', 'B2', 'C1']\n",
    "new_table = pd.DataFrame(columns=['Sentence', 'Level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in levels:\n",
    "    level_rows = movies_df[movies_df['Level'] == level].head(num_rows_per_level)\n",
    "    new_table = new_table.append(level_rows, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = new_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B1    37000\n",
      "B2    37000\n",
      "C1    18595\n",
      "A2     2630\n",
      "Name: Level, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(movies_df['Level'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_lemmas_by_level(df):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    levels = ['A1', 'A2', 'B1', 'B2', 'C1']\n",
    "    unique_lemmas_by_level = {level: [] for level in levels}\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        text = row['Sentence'].lower()\n",
    "        level = row['Level']\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        unique_lemmas = len(set(lemmas))\n",
    "        unique_lemmas_by_level[level].append(unique_lemmas)\n",
    "    \n",
    "    return unique_lemmas_by_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вызов функции get_unique_lemmas_by_level и получение словаря с количеством уникальных лемм для каждого уровня\n",
    "unique_lemmas = get_unique_lemmas_by_level(movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определим словарь меток с кодированием значений\n",
    "label_dict = {'A2': 1,\n",
    "              'B1': 2,\n",
    "              'B2': 3,\n",
    "              'C1': 4}\n",
    "# заменим метки числовыми значениями\n",
    "movies_df = movies_df.replace(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для подсчета слов каждого уровня\n",
    "def count_word_levels(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "    level_counts = {\"A2\": 0, \"B1\": 0, \"B2\": 0, \"C1\": 0}\n",
    "\n",
    "    for word, tag in pos_tags:\n",
    "        if tag.startswith(\"NN\"):\n",
    "            level_counts[\"A2\"] += 1\n",
    "        elif tag.startswith(\"VB\"):\n",
    "            level_counts[\"B1\"] += 1\n",
    "        elif tag.startswith(\"JJ\"):\n",
    "            level_counts[\"B2\"] += 1\n",
    "        elif tag.startswith(\"RB\"):\n",
    "            level_counts[\"C1\"] += 1\n",
    "\n",
    "    return level_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_3572\\1894916258.py:4: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  movies_df[column_name] = pd.Series(unique_lemmas_level).fillna(0)\n"
     ]
    }
   ],
   "source": [
    "for level in unique_lemmas:\n",
    "    column_name = f'{level}_unique_lemmas'\n",
    "    unique_lemmas_level = unique_lemmas[level]\n",
    "    movies_df[column_name] = pd.Series(unique_lemmas_level).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_count(lemmas, oxf, cat):\n",
    "    func_dict = {'A1': 0,\n",
    "                 'A2': 1,\n",
    "                 'B1': 2,\n",
    "                 'B2': 3,\n",
    "                 'C1': 4}\n",
    "    level = func_dict[cat]\n",
    "    oxf_word_list = oxf[level].split()\n",
    "    words = [lemma for lemma in lemmas if lemma in oxf_word_list]\n",
    "\n",
    "    return len(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для токенизации предложения\n",
    "def tokenize_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применение функции count_word_levels для подсчета слов каждого уровня в предложениях\n",
    "movies_df[[\"A2\", \"B1\", \"B2\", \"C1\"]] = pd.DataFrame(movies_df[\"Sentence\"].apply(count_word_levels).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для подсчета количества существительных в предложении\n",
    "def count_nouns(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    noun_count = len([word for word, pos in tagged if pos.startswith('NN')])\n",
    "    return noun_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df['Noun_Count'] = movies_df['Sentence'].apply(count_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    if text == text:\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        words = word_tokenize(text)\n",
    "        words = [word for word in words if word.casefold() not in all_stopwords]\n",
    "\n",
    "        return \" \".join([porter.stem(word) for word in words])\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = movies_df.drop('A1_unique_lemmas', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Level</th>\n",
       "      <th>Movie</th>\n",
       "      <th>A2_unique_lemmas</th>\n",
       "      <th>B1_unique_lemmas</th>\n",
       "      <th>B2_unique_lemmas</th>\n",
       "      <th>C1_unique_lemmas</th>\n",
       "      <th>A2</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>C1</th>\n",
       "      <th>Noun_Count</th>\n",
       "      <th>porter_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>little girl</td>\n",
       "      <td>1</td>\n",
       "      <td>The Walking Dead-S01E01-Days Gone Bye.English</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>littl girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>don't be afraid, okay? little girl</td>\n",
       "      <td>1</td>\n",
       "      <td>The Walking Dead-S01E01-Days Gone Bye.English</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>dont afraid okay littl girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>panting growling oh my god</td>\n",
       "      <td>1</td>\n",
       "      <td>The Walking Dead-S01E01-Days Gone Bye.English</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>pant growl oh god</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>theme music playing police radio chatter man w...</td>\n",
       "      <td>1</td>\n",
       "      <td>The Walking Dead-S01E01-Days Gone Bye.English</td>\n",
       "      <td>24.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>theme music play polic radio chatter man what ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>man i never met a woman who knew how to turn o...</td>\n",
       "      <td>1</td>\n",
       "      <td>The Walking Dead-S01E01-Days Gone Bye.English</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>man never met woman knew turn light</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Level  \\\n",
       "0                                        little girl      1   \n",
       "1                 don't be afraid, okay? little girl      1   \n",
       "2                         panting growling oh my god      1   \n",
       "3  theme music playing police radio chatter man w...      1   \n",
       "4  man i never met a woman who knew how to turn o...      1   \n",
       "\n",
       "                                           Movie  A2_unique_lemmas  \\\n",
       "0  The Walking Dead-S01E01-Days Gone Bye.English               2.0   \n",
       "1  The Walking Dead-S01E01-Days Gone Bye.English               9.0   \n",
       "2  The Walking Dead-S01E01-Days Gone Bye.English               5.0   \n",
       "3  The Walking Dead-S01E01-Days Gone Bye.English              24.0   \n",
       "4  The Walking Dead-S01E01-Days Gone Bye.English              13.0   \n",
       "\n",
       "   B1_unique_lemmas  B2_unique_lemmas  C1_unique_lemmas  A2  B1  B2  C1  \\\n",
       "0              15.0              24.0               6.0   1   0   1   0   \n",
       "1               4.0              11.0               7.0   1   3   2   1   \n",
       "2              10.0              14.0               4.0   1   2   1   0   \n",
       "3               9.0              14.0               8.0  17   4   1   0   \n",
       "4               8.0               6.0              12.0   3   4   0   1   \n",
       "\n",
       "   Noun_Count                                        porter_text  \n",
       "0           1                                         littl girl  \n",
       "1           1                        dont afraid okay littl girl  \n",
       "2           1                                  pant growl oh god  \n",
       "3          17  theme music play polic radio chatter man what ...  \n",
       "4           3                man never met woman knew turn light  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# выполняем нормализацию данных\n",
    "movies_df['porter_text'] = movies_df['Sentence'].apply(tokenizer_porter)\n",
    "\n",
    "movies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = movies_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Level</th>\n",
       "      <th>Movie</th>\n",
       "      <th>A2_unique_lemmas</th>\n",
       "      <th>B1_unique_lemmas</th>\n",
       "      <th>B2_unique_lemmas</th>\n",
       "      <th>C1_unique_lemmas</th>\n",
       "      <th>A2</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>C1</th>\n",
       "      <th>Noun_Count</th>\n",
       "      <th>porter_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>little girl</td>\n",
       "      <td>1</td>\n",
       "      <td>The Walking Dead-S01E01-Days Gone Bye.English</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>littl girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>don't be afraid, okay? little girl</td>\n",
       "      <td>1</td>\n",
       "      <td>The Walking Dead-S01E01-Days Gone Bye.English</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>dont afraid okay littl girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>panting growling oh my god</td>\n",
       "      <td>1</td>\n",
       "      <td>The Walking Dead-S01E01-Days Gone Bye.English</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>pant growl oh god</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>theme music playing police radio chatter man w...</td>\n",
       "      <td>1</td>\n",
       "      <td>The Walking Dead-S01E01-Days Gone Bye.English</td>\n",
       "      <td>24.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>theme music play polic radio chatter man what ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>man i never met a woman who knew how to turn o...</td>\n",
       "      <td>1</td>\n",
       "      <td>The Walking Dead-S01E01-Days Gone Bye.English</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>man never met woman knew turn light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95220</th>\n",
       "      <td>every charge i'm about to bring against your m...</td>\n",
       "      <td>4</td>\n",
       "      <td>The_Legend_of_Tarzan(2016)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>everi charg im bring majesti person govern con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95221</th>\n",
       "      <td>we have witnesses, documents, letters, and off...</td>\n",
       "      <td>4</td>\n",
       "      <td>The_Legend_of_Tarzan(2016)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>wit document letter offici record bring attent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95222</th>\n",
       "      <td>july th,</td>\n",
       "      <td>4</td>\n",
       "      <td>The_Legend_of_Tarzan(2016)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>juli th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95223</th>\n",
       "      <td>jane they are singing the legend of tarzan</td>\n",
       "      <td>4</td>\n",
       "      <td>The_Legend_of_Tarzan(2016)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>jane sing legend tarzan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95224</th>\n",
       "      <td>he understood them, and learned to be as one w...</td>\n",
       "      <td>4</td>\n",
       "      <td>The_Legend_of_Tarzan(2016)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>understood learn one</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95225 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sentence  Level  \\\n",
       "0                                            little girl      1   \n",
       "1                     don't be afraid, okay? little girl      1   \n",
       "2                             panting growling oh my god      1   \n",
       "3      theme music playing police radio chatter man w...      1   \n",
       "4      man i never met a woman who knew how to turn o...      1   \n",
       "...                                                  ...    ...   \n",
       "95220  every charge i'm about to bring against your m...      4   \n",
       "95221  we have witnesses, documents, letters, and off...      4   \n",
       "95222                                          july th,       4   \n",
       "95223         jane they are singing the legend of tarzan      4   \n",
       "95224  he understood them, and learned to be as one w...      4   \n",
       "\n",
       "                                               Movie  A2_unique_lemmas  \\\n",
       "0      The Walking Dead-S01E01-Days Gone Bye.English               2.0   \n",
       "1      The Walking Dead-S01E01-Days Gone Bye.English               9.0   \n",
       "2      The Walking Dead-S01E01-Days Gone Bye.English               5.0   \n",
       "3      The Walking Dead-S01E01-Days Gone Bye.English              24.0   \n",
       "4      The Walking Dead-S01E01-Days Gone Bye.English              13.0   \n",
       "...                                              ...               ...   \n",
       "95220                     The_Legend_of_Tarzan(2016)               0.0   \n",
       "95221                     The_Legend_of_Tarzan(2016)               0.0   \n",
       "95222                     The_Legend_of_Tarzan(2016)               0.0   \n",
       "95223                     The_Legend_of_Tarzan(2016)               0.0   \n",
       "95224                     The_Legend_of_Tarzan(2016)               0.0   \n",
       "\n",
       "       B1_unique_lemmas  B2_unique_lemmas  C1_unique_lemmas  A2  B1  B2  C1  \\\n",
       "0                  15.0              24.0               6.0   1   0   1   0   \n",
       "1                   4.0              11.0               7.0   1   3   2   1   \n",
       "2                  10.0              14.0               4.0   1   2   1   0   \n",
       "3                   9.0              14.0               8.0  17   4   1   0   \n",
       "4                   8.0               6.0              12.0   3   4   0   1   \n",
       "...                 ...               ...               ...  ..  ..  ..  ..   \n",
       "95220               0.0               0.0               0.0   4   5   2   1   \n",
       "95221               0.0               0.0               0.0   9   3   3   0   \n",
       "95222               0.0               0.0               0.0   2   0   0   0   \n",
       "95223               0.0               0.0               0.0   3   2   0   0   \n",
       "95224               0.0               0.0               0.0   0   3   0   0   \n",
       "\n",
       "       Noun_Count                                        porter_text  \n",
       "0               1                                         littl girl  \n",
       "1               1                        dont afraid okay littl girl  \n",
       "2               1                                  pant growl oh god  \n",
       "3              17  theme music play polic radio chatter man what ...  \n",
       "4               3                man never met woman knew turn light  \n",
       "...           ...                                                ...  \n",
       "95220           4  everi charg im bring majesti person govern con...  \n",
       "95221           9  wit document letter offici record bring attent...  \n",
       "95222           2                                            juli th  \n",
       "95223           3                            jane sing legend tarzan  \n",
       "95224           0                               understood learn one  \n",
       "\n",
       "[95225 rows x 13 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучающая выборка: 71418\n",
      "Валидационная выборка: 23807\n"
     ]
    }
   ],
   "source": [
    "features = movies_df[['porter_text', 'A2', 'B1', 'B2', 'C1', 'A2_unique_lemmas', 'B1_unique_lemmas', 'B2_unique_lemmas', 'C1_unique_lemmas', 'Noun_Count']]\n",
    "target = movies_df['Level']\n",
    "\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(features, target, test_size=0.25, random_state=12345, shuffle=True)\n",
    "\n",
    "print(f'Обучающая выборка:', features_train.shape[0])\n",
    "print(f'Валидационная выборка:', features_valid.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = ['A2', 'B1', 'B2', 'C1']\n",
    "\n",
    "features_train_ohe = features_train.copy()\n",
    "features_valid_ohe = features_valid.copy()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "features_train_ohe[numeric] = scaler.fit_transform(features_train_ohe[numeric])\n",
    "features_valid_ohe[numeric] = scaler.transform(features_valid_ohe[numeric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# используем викторизацию\n",
    "tfidf = TfidfVectorizer(lowercase=False, max_features=10000, ngram_range=(1, 3))\n",
    "column_transformer = ColumnTransformer([('vect1', tfidf, 'porter_text')], remainder='passthrough')\n",
    "\n",
    "features_train_ohe = column_transformer.fit_transform(features_train_ohe)\n",
    "features_valid_ohe = column_transformer.transform(features_valid_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "best_cls = None\n",
    "best_params = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(clf, params, name):\n",
    "    print(\"=\" * 80)\n",
    "    print(name)\n",
    "    print(params)\n",
    "    \n",
    "    start_time = time()\n",
    "    \n",
    "    gs_clf = GridSearchCV(clf, params, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n",
    "    gs_clf.fit(features_train_ohe, target_train)\n",
    "    \n",
    "    train_time = time() - start_time\n",
    "    print(f\"Train time: {train_time:.3}s\")\n",
    "\n",
    "    start_time = time()\n",
    "    \n",
    "    best_clf = gs_clf.best_estimator_\n",
    "    pred = best_clf.predict(features_valid_ohe)\n",
    "    test_time = time() - start_time\n",
    "    print(f\"Test time:  {test_time:.3}s\")\n",
    "\n",
    "    score = metrics.f1_score(target_valid, pred, average='weighted')\n",
    "    \n",
    "    global best_cls, best_score, best_params\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_cls = best_clf\n",
    "        best_score = score\n",
    "        best_params = gs_clf.best_params_\n",
    "    \n",
    "    print(f\"Valid accuracy:   {score:.3}\")\n",
    "    \n",
    "    if hasattr(clf, \"coef_\"):\n",
    "        print(f\"Dimensionality: {best_clf.coef_.shape[1]}\")\n",
    "        print(f\"Density: {best_clf.coef_.density}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    return name, score, train_time, test_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Linear SVC\n",
      "{'C': [0.1, 0.5, 1.0]}\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Train time: 40.9s\n",
      "Test time:  0.006s\n",
      "Valid accuracy:   0.816\n",
      "\n",
      "================================================================================\n",
      "SGD\n",
      "{'n_iter_no_change': [3, 5, 7], 'loss': ['hinge', 'log_loss', 'log', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']}\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Train time: 49.2s\n",
      "Test time:  0.004s\n",
      "Valid accuracy:   0.802\n",
      "\n",
      "================================================================================\n",
      "LightGBM\n",
      "{'n_estimators': [50, 100, 200], 'learning_rate': [0.1, 0.01, 0.001]}\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.682583 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 84085\n",
      "[LightGBM] [Info] Number of data points in the train set: 71418, number of used features: 2663\n",
      "[LightGBM] [Info] Start training from score -3.608960\n",
      "[LightGBM] [Info] Start training from score -0.943946\n",
      "[LightGBM] [Info] Start training from score -0.945278\n",
      "[LightGBM] [Info] Start training from score -1.633429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Train time: 6.16e+02s\n",
      "Test time:  0.575s\n",
      "Valid accuracy:   0.789\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "STATE = 12345\n",
    "\n",
    "classifiers = [\n",
    "    (LinearSVC(class_weight='balanced', loss='squared_hinge', dual=False, random_state=STATE), \n",
    "     {'C': [0.1, 0.5, 1.0]}, \"Linear SVC\"),\n",
    "    (SGDClassifier(alpha=1e-4, early_stopping=True, class_weight='balanced', random_state=STATE, n_jobs=-1), \n",
    "     {'n_iter_no_change': [3, 5, 7], 'loss': ['hinge', 'log_loss', 'log', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']}, \"SGD\"),\n",
    "    (LGBMClassifier(random_state=STATE, verbose=1), {'n_estimators': [50, 100, 200], 'learning_rate': [0.1, 0.01, 0.001]}, \"LightGBM\")\n",
    "]\n",
    "\n",
    "for clf, params, name in classifiers:\n",
    "    results.append(benchmark(clf, params, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Результаты:\n",
      "Модель: Linear SVC\n",
      "Valid F1: 0.816\n",
      "Train time: 40.883s\n",
      "Test time: 0.006s\n",
      "\n",
      "Модель: SGD\n",
      "Valid F1: 0.802\n",
      "Train time: 49.221s\n",
      "Test time: 0.004s\n",
      "\n",
      "Модель: LightGBM\n",
      "Valid F1: 0.789\n",
      "Train time: 616.083s\n",
      "Test time: 0.575s\n",
      "\n",
      "================================================================================\n",
      "Наилучшая модель:\n",
      "Модель: LinearSVC\n",
      "Valid F1: 0.816\n",
      "Лучшие параметры: {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Вывод результатов\n",
    "print(\"=\" * 80)\n",
    "print(\"Результаты:\")\n",
    "for result in results:\n",
    "    name, score, train_time, test_time = result\n",
    "    print(f\"Модель: {name}\")\n",
    "    print(f\"Valid F1: {score:.3f}\")\n",
    "    print(f\"Train time: {train_time:.3f}s\")\n",
    "    print(f\"Test time: {test_time:.3f}s\")\n",
    "    print()\n",
    "\n",
    "# Вывод наилучшей модели\n",
    "print(\"=\" * 80)\n",
    "print(\"Наилучшая модель:\")\n",
    "print(f\"Модель: {best_cls.__class__.__name__}\")\n",
    "print(f\"Valid F1: {best_score:.3f}\")\n",
    "print(\"Лучшие параметры:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['linear_svc_model.joblib']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(best_cls, 'linear_svc_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование разреженной матрицы в плотную\n",
    "features_train_dense = features_train_ohe.toarray()\n",
    "\n",
    "# Вычисление перестановочной важности\n",
    "result = permutation_importance(best_cls, features_train_dense, target_train, n_repeats=10, random_state=42)\n",
    "\n",
    "# Получение отсортированных признаков по важности\n",
    "sorted_indices = result.importances_mean.argsort()[::-1]\n",
    "sorted_feature_names = [feature_names[i] for i in sorted_indices]\n",
    "sorted_importances = result.importances_mean[sorted_indices]\n",
    "\n",
    "# Вывод важности признаков\n",
    "for feature_name, importance in zip(sorted_feature_names, sorted_importances):\n",
    "    print(f\"{feature_name}: {importance}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исходя из результатов, лучшей моделью оказалась модель LinearSVC с валидационной оценкой F1 0.816. Она достигла этой оценки при использовании параметра C равного 0.1.\n",
    "\n",
    "Время обучения модели LinearSVC составило 37.927 секунд, что является относительно небольшим временем для обучения. Время тестирования модели было всего 0.011 секунд.\n",
    "\n",
    "С другой стороны, модель SGD также показала хорошие результаты с валидационной оценкой F1 0.802. Время обучения для модели SGD составило 47.084 секунды, а время тестирования было 0.004 секунды.\n",
    "\n",
    "Модель LightGBM демонстрирует немного более низкую валидационную оценку F1 в 0.789, однако она обладает наиболее длительным временем обучения, равным 623.204 секунды, и временем тестирования, составляющим 0.593 секунды.\n",
    "\n",
    "Исходя из этих результатов, можно заключить, что модель LinearSVC с параметром C равным 0.1 является наилучшей моделью с высокой оценкой F1 и относительно небольшим временем обучения и тестирования."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
